{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI Conversors.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EliasParada/Programacion_Computacional_III/blob/AI_Conversors/AI_Conversors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gE4jPz80iOS3"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-w4cHFM0831u"
      },
      "source": [
        "inputs = pd.read_csv(\"/content/entrenamiento.csv\", sep=\";\")\n",
        "\n",
        "#For Length, like meters to kilometers\n",
        "length = inputs[inputs.Types == \"Length\"]\n",
        "#For Mass, like grams to libras\n",
        "mass = inputs[inputs.Types == \"Mass\"]\n",
        "#For Storage, like kilobytes to megabytes\n",
        "storage = inputs[inputs.Types == \"Storage\"]\n",
        "#Fro Time, like seconds to minutes\n",
        "time = inputs[inputs.Types == \"Time\"]\n",
        "#For Area, like square meter to square meter\n",
        "area = inputs[inputs.Types == \"Area\"]\n",
        "#For acumulatted months\n",
        "accMonth = inputs[inputs.Types == \"Accum\"]\n",
        "\n",
        "# sb.scatterplot(length['Value_A'], length['Value_B'])\n",
        "# plt.show()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHP9qbCNXJfj"
      },
      "source": [
        "#Length\n",
        "centi, meter, kilometer, mile, yard, foot = length['Value_A'], length['Value_B'], length['Value_C'], length['Value_D'], length['Value_E'], length['Value_F']\n",
        "\n",
        "#Mass\n",
        "gram, kilogram, ton, stone, libra, once = mass['Value_A'], mass['Value_B'], mass['Value_C'], mass['Value_D'], mass['Value_E'], mass['Value_F']\n",
        "\n",
        "#Storage\n",
        "byte, kilobyte, megabyte, gigabyte, terabyte, petabyte = storage['Value_A'], storage['Value_B'], storage['Value_C'], storage['Value_D'], storage['Value_E'], storage['Value_F']\n",
        "\n",
        "#Time\n",
        "second, minute, hour, day, week, month = time['Value_A'], time['Value_B'], time['Value_C'], time['Value_D'], time['Value_E'], time['Value_F']\n",
        "\n",
        "#Area\n",
        "sqrMeter, sqrKilometer, sqrHectare, sqrMile, sqrYard, sqrFoot = area['Value_A'], area['Value_B'], area['Value_C'], area['Value_D'], area['Value_E'], area['Value_F']\n",
        "\n",
        "#Accumulated Months\n",
        "inputMonth, accumMonth = accMonth['Value_A'], accMonth['Value_B']"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3T7z6BnPyHS7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aec57d3e-818e-4684-f44c-7805a4f3416a"
      },
      "source": [
        "#Create models\n",
        "try:\n",
        "  #Equal\n",
        "  modelEqual = tf.keras.Sequential()\n",
        "  modelEqual.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  #Length\n",
        "  ####Centimeters\n",
        "  modelCentMet = tf.keras.Sequential()\n",
        "  modelCentMet.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelCentKilomet = tf.keras.Sequential()\n",
        "  modelCentKilomet.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelCentMile = tf.keras.Sequential()\n",
        "  modelCentMile.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelCentYard = tf.keras.Sequential()\n",
        "  modelCentYard.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelCentFoot = tf.keras.Sequential()\n",
        "  modelCentFoot.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  ####Meters\n",
        "  modelMetCent = tf.keras.Sequential()\n",
        "  modelMetCent.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelMetKilomet = tf.keras.Sequential()\n",
        "  modelMetKilomet.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelMetMile = tf.keras.Sequential()\n",
        "  modelMetMile.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelMetYard = tf.keras.Sequential()\n",
        "  modelMetYard.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelMetFoot = tf.keras.Sequential()\n",
        "  modelMetFoot.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  ####Kilometers\n",
        "  modelKilometCent = tf.keras.Sequential()\n",
        "  modelKilometCent.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelKilometMet = tf.keras.Sequential()\n",
        "  modelKilometMet.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelKilometMile = tf.keras.Sequential()\n",
        "  modelKilometMile.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelKilometYard = tf.keras.Sequential()\n",
        "  modelKilometYard.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelKilometFoot = tf.keras.Sequential()\n",
        "  modelKilometFoot.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  ####Miles\n",
        "  modelMileCent = tf.keras.Sequential()\n",
        "  modelMileCent.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelMileMet = tf.keras.Sequential()\n",
        "  modelMileMet.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelMileKilomet = tf.keras.Sequential()\n",
        "  modelMileKilomet.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelMileYard = tf.keras.Sequential()\n",
        "  modelMileYard.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelMileFoot = tf.keras.Sequential()\n",
        "  modelMileFoot.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  ####Yards\n",
        "  modelYardCent = tf.keras.Sequential()\n",
        "  modelYardCent.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelYardMet = tf.keras.Sequential()\n",
        "  modelYardMet.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelYardKilomet = tf.keras.Sequential()\n",
        "  modelYardKilomet.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelYardMile = tf.keras.Sequential()\n",
        "  modelYardMile.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelYardFoot = tf.keras.Sequential()\n",
        "  modelYardFoot.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  ####Foots\n",
        "  modelFootCent = tf.keras.Sequential()\n",
        "  modelFootCent.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelFootMet = tf.keras.Sequential()\n",
        "  modelFootMet.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelFootKilomet = tf.keras.Sequential()\n",
        "  modelFootKilomet.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelFootMile = tf.keras.Sequential()\n",
        "  modelFootMile.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelFootYard = tf.keras.Sequential()\n",
        "  modelFootYard.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "\n",
        "  #Mass\n",
        "  ####Grams\n",
        "  modelGramKilog = tf.keras.Sequential()\n",
        "  modelGramKilog.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelGramTon = tf.keras.Sequential()\n",
        "  modelGramTon.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelGramSton = tf.keras.Sequential()\n",
        "  modelGramSton.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelGramLibra = tf.keras.Sequential()\n",
        "  modelGramLibra.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelGramOce = tf.keras.Sequential()\n",
        "  modelGramOce.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  ####Kilograms\n",
        "  modelKilogGram = tf.keras.Sequential()\n",
        "  modelKilogGram.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelKilogTon = tf.keras.Sequential()\n",
        "  modelKilogTon.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelKilogSton = tf.keras.Sequential()\n",
        "  modelKilogSton.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelKilogLibra = tf.keras.Sequential()\n",
        "  modelKilogLibra.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelKilogOnce = tf.keras.Sequential()\n",
        "  modelKilogOnce.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  ####Tons\n",
        "  modelTonGram = tf.keras.Sequential()\n",
        "  modelTonGram.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelTonKilog = tf.keras.Sequential()\n",
        "  modelTonKilog.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelTonSton = tf.keras.Sequential()\n",
        "  modelTonSton.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelTonLibra = tf.keras.Sequential()\n",
        "  modelTonLibra.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelTonOnce = tf.keras.Sequential()\n",
        "  modelTonOnce.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  ####Stones\n",
        "  modelStoneGram = tf.keras.Sequential()\n",
        "  modelStoneGram.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelStoneKilog = tf.keras.Sequential()\n",
        "  modelStoneKilog.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelStoneTon = tf.keras.Sequential()\n",
        "  modelStoneTon.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelStoneLibra = tf.keras.Sequential()\n",
        "  modelStoneLibra.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelStoneOnce = tf.keras.Sequential()\n",
        "  modelStoneOnce.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  ####Libras\n",
        "  modelLibraGram = tf.keras.Sequential()\n",
        "  modelLibraGram.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelLibraKilog = tf.keras.Sequential()\n",
        "  modelLibraKilog.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelLibraTon = tf.keras.Sequential()\n",
        "  modelLibraTon.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelLibraSton = tf.keras.Sequential()\n",
        "  modelLibraSton.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelLibraOnce = tf.keras.Sequential()\n",
        "  modelLibraOnce.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  ####Onces\n",
        "  modelOnceGram = tf.keras.Sequential()\n",
        "  modelOnceGram.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelOnceKilog = tf.keras.Sequential()\n",
        "  modelOnceKilog.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelOnceTon = tf.keras.Sequential()\n",
        "  modelOnceTon.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelOnceSton = tf.keras.Sequential()\n",
        "  modelOnceSton.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelOnceLibra = tf.keras.Sequential()\n",
        "  modelOnceLibra.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "\n",
        "  #Storage\n",
        "  ####Bytes\n",
        "  modelBKb = tf.keras.Sequential()\n",
        "  modelBKb.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelBMb = tf.keras.Sequential()\n",
        "  modelBMb.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelBGb = tf.keras.Sequential()\n",
        "  modelBGb.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelBTb = tf.keras.Sequential()\n",
        "  modelBTb.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelBPb = tf.keras.Sequential()\n",
        "  modelBPb.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  ####Kilobytes\n",
        "  modelKbB = tf.keras.Sequential()\n",
        "  modelKbB.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelKbMb = tf.keras.Sequential()\n",
        "  modelKbMb.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelKbGb = tf.keras.Sequential()\n",
        "  modelKbGb.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelKbTb = tf.keras.Sequential()\n",
        "  modelKbTb.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelKbPb = tf.keras.Sequential()\n",
        "  modelKbPb.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  ####Megabytes\n",
        "  modelMbB = tf.keras.Sequential()\n",
        "  modelMbB.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelMbKb = tf.keras.Sequential()\n",
        "  modelMbKb.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelMbGb = tf.keras.Sequential()\n",
        "  modelMbGb.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelMbTb = tf.keras.Sequential()\n",
        "  modelMbTb.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelMbPb = tf.keras.Sequential()\n",
        "  modelMbPb.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  ####Gigabytes\n",
        "  modelGbB = tf.keras.Sequential()\n",
        "  modelGbB.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelGbKb = tf.keras.Sequential()\n",
        "  modelGbKb.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelGbMb = tf.keras.Sequential()\n",
        "  modelGbMb.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelGbTb = tf.keras.Sequential()\n",
        "  modelGbTb.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelGbPb = tf.keras.Sequential()\n",
        "  modelGbPb.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  ####Terabytes\n",
        "  modelTbB = tf.keras.Sequential()\n",
        "  modelTbB.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelTbKb = tf.keras.Sequential()\n",
        "  modelTbKb.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelTbMb = tf.keras.Sequential()\n",
        "  modelTbMb.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelTbGb = tf.keras.Sequential()\n",
        "  modelTbGb.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelTbPb = tf.keras.Sequential()\n",
        "  modelTbPb.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  ####Petabytes\n",
        "  modelPbB = tf.keras.Sequential()\n",
        "  modelPbB.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelPbKb = tf.keras.Sequential()\n",
        "  modelPbKb.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelPbMb = tf.keras.Sequential()\n",
        "  modelPbMb.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelPbGb = tf.keras.Sequential()\n",
        "  modelPbGb.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelPbTb = tf.keras.Sequential()\n",
        "  modelPbTb.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "\n",
        "  #Time\n",
        "  ####Seconds\n",
        "  modelSecMin = tf.keras.Sequential()\n",
        "  modelSecMin.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelSecHour = tf.keras.Sequential()\n",
        "  modelSecHour.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelSecDay = tf.keras.Sequential()\n",
        "  modelSecDay.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelSecWeek = tf.keras.Sequential()\n",
        "  modelSecWeek.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelSecMonth = tf.keras.Sequential()\n",
        "  modelSecMonth.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  ####Minutes\n",
        "  modelMinSec = tf.keras.Sequential()\n",
        "  modelMinSec.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelMinHour = tf.keras.Sequential()\n",
        "  modelMinHour.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelMinDay = tf.keras.Sequential()\n",
        "  modelMinDay.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelMinWeek = tf.keras.Sequential()\n",
        "  modelMinWeek.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelMinMonth = tf.keras.Sequential()\n",
        "  modelMinMonth.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  ####Hours\n",
        "  modelHourSec = tf.keras.Sequential()\n",
        "  modelHourSec.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelHourMin = tf.keras.Sequential()\n",
        "  modelHourMin.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelHourDay = tf.keras.Sequential()\n",
        "  modelHourDay.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelHourWeek = tf.keras.Sequential()\n",
        "  modelHourWeek.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelHourMonth = tf.keras.Sequential()\n",
        "  modelHourMonth.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  ####Days\n",
        "  modelDaySec = tf.keras.Sequential()\n",
        "  modelDaySec.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelDayMin = tf.keras.Sequential()\n",
        "  modelDayMin.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelDayHour = tf.keras.Sequential()\n",
        "  modelDayHour.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelDayWeek = tf.keras.Sequential()\n",
        "  modelDayWeek.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelDayMonth = tf.keras.Sequential()\n",
        "  modelDayMonth.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  ####Weeks\n",
        "  modelWeekSec = tf.keras.Sequential()\n",
        "  modelWeekSec.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelWeekMin = tf.keras.Sequential()\n",
        "  modelWeekMin.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelWeekHour = tf.keras.Sequential()\n",
        "  modelWeekHour.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelWeekDay = tf.keras.Sequential()\n",
        "  modelWeekDay.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelWeekMonth = tf.keras.Sequential()\n",
        "  modelWeekMonth.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  ####Month\n",
        "  modelMonthSec = tf.keras.Sequential()\n",
        "  modelMonthSec.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelMonthMin = tf.keras.Sequential()\n",
        "  modelMonthMin.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelMonthHour = tf.keras.Sequential()\n",
        "  modelMonthHour.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelMonthDay = tf.keras.Sequential()\n",
        "  modelMonthDay.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelMonthWeek = tf.keras.Sequential()\n",
        "  modelMonthWeek.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "\n",
        "  #Area\n",
        "  ####Squere Meter\n",
        "  modelSqrmSqrk = tf.keras.Sequential()\n",
        "  modelSqrmSqrk.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelSqrmSqrh = tf.keras.Sequential()\n",
        "  modelSqrmSqrh.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelSqrmSqrml = tf.keras.Sequential()\n",
        "  modelSqrmSqrml.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelSqrmSqry = tf.keras.Sequential()\n",
        "  modelSqrmSqry.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelSqrmSqrf = tf.keras.Sequential()\n",
        "  modelSqrmSqrf.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  ####Squere Kilometers\n",
        "  modelSqrkSqrm = tf.keras.Sequential()\n",
        "  modelSqrkSqrm.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelSqrkSqrh = tf.keras.Sequential()\n",
        "  modelSqrkSqrh.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelSqrkSqrml = tf.keras.Sequential()\n",
        "  modelSqrkSqrml.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelSqrkSqry = tf.keras.Sequential()\n",
        "  modelSqrkSqry.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelSqrkSqrf = tf.keras.Sequential()\n",
        "  modelSqrkSqrf.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  ####Hectares\n",
        "  modelSqrhSqrm = tf.keras.Sequential()\n",
        "  modelSqrhSqrm.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelSqrhSqrkm = tf.keras.Sequential()\n",
        "  modelSqrhSqrkm.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelSqrhSqrml = tf.keras.Sequential()\n",
        "  modelSqrhSqrml.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelSqrhSqry = tf.keras.Sequential()\n",
        "  modelSqrhSqry.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelSqrhSqrf = tf.keras.Sequential()\n",
        "  modelSqrhSqrf.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  ####Square Miles\n",
        "  modelSqrmlSqrm = tf.keras.Sequential()\n",
        "  modelSqrmlSqrm.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelSqrmlSqrkm = tf.keras.Sequential()\n",
        "  modelSqrmlSqrkm.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelSqrmlSqrh = tf.keras.Sequential()\n",
        "  modelSqrmlSqrh.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelSqrmlSqry = tf.keras.Sequential()\n",
        "  modelSqrmlSqry.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelSqrmlSqrf = tf.keras.Sequential()\n",
        "  modelSqrmlSqrf.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  ####Square Yards\n",
        "  modelSqrySqrm = tf.keras.Sequential()\n",
        "  modelSqrySqrm.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelSqrySqrkm = tf.keras.Sequential()\n",
        "  modelSqrySqrkm.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelSqrySqrh = tf.keras.Sequential()\n",
        "  modelSqrySqrh.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelSqrySqrml = tf.keras.Sequential()\n",
        "  modelSqrySqrml.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelSqrySqrf = tf.keras.Sequential()\n",
        "  modelSqrySqrf.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  ####Square Foots\n",
        "  modelSqrfSqrm = tf.keras.Sequential()\n",
        "  modelSqrfSqrm.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelSqrfSqrkm = tf.keras.Sequential()\n",
        "  modelSqrfSqrkm.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelSqrfSqrh = tf.keras.Sequential()\n",
        "  modelSqrfSqrh.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelSqrfSqrml = tf.keras.Sequential()\n",
        "  modelSqrfSqrml.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "  modelSqrfSqry = tf.keras.Sequential()\n",
        "  modelSqrfSqry.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "\n",
        "  #Accumulated months\n",
        "  modelAccMonth = tf.keras.Sequential()\n",
        "  modelAccMonth.add(tf.keras.layers.Dense(units=1, input_shape=[1]))\n",
        "\n",
        "  print('\\033[1;42m\\033[37m ¡Modelos creados correctamente! \\033[0m')\n",
        "except:\n",
        "  print('\\033[1;41m\\033[1;37m ¡Ha ocurrido un error o se interrumpio la acción! \\033[0m')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;42m\u001b[37m ¡Modelos creados correctamente! \u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o36tOvG92WZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91db1c6b-35ed-4a16-eb83-bdbf53140ead"
      },
      "source": [
        "#Compile the models\n",
        "try:\n",
        "  #Equal\n",
        "  modelEqual.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "\n",
        "  #Length\n",
        "  ####Centimeters\n",
        "  modelCentMet.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelCentKilomet.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelCentMile.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelCentYard.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelCentFoot.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  ####Meters\n",
        "  modelMetCent.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelMetKilomet.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelMetMile.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelMetYard.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelMetFoot.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  ####Kilometers\n",
        "  modelKilometCent.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelKilometMet.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelKilometMile.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelKilometYard.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelKilometFoot.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  ####Miles\n",
        "  modelMileCent.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelMileMet.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelMileKilomet.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelMileYard.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelMileFoot.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  ####Yard\n",
        "  modelYardCent.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelYardMet.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelYardKilomet.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelYardMile.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelYardFoot.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  ####Foot\n",
        "  modelFootCent.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelFootMet.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelFootKilomet.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelFootMile.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelFootYard.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "\n",
        "  #Mass\n",
        "  ####Grams\n",
        "  modelGramKilog.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelGramTon.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelGramSton.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelGramLibra.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelGramOce.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  ####Kilograms\n",
        "  modelKilogGram.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelKilogTon.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelKilogSton.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelKilogLibra.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelKilogOnce.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  ####Tons\n",
        "  modelTonGram.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelTonKilog.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelTonSton.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelTonLibra.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelTonOnce.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  ####Stones\n",
        "  modelStoneGram.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelStoneKilog.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelStoneTon.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelStoneLibra.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelStoneOnce.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  ####Libras\n",
        "  modelLibraGram.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelLibraKilog.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelLibraTon.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelLibraSton.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelLibraOnce.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  ####Onces\n",
        "  modelOnceGram.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelOnceKilog.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelOnceTon.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelOnceSton.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelOnceLibra.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "\n",
        "  #Storage\n",
        "  ####Bytes\n",
        "  modelBKb.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelBMb.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelBGb.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelBTb.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelBPb.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  ####Kilobytes\n",
        "  modelKbB.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelKbMb.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelKbGb.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelKbTb.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelKbPb.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  ####Megabytes\n",
        "  modelMbB.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelMbKb.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelMbGb.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelMbTb.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelMbPb.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  ####Gigabytes\n",
        "  modelGbB.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelGbKb.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelGbMb.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelGbTb.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelGbPb.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  ####Terabytes\n",
        "  modelTbB.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelTbKb.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelTbMb.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelTbGb.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelTbPb.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  ####Petabytes\n",
        "  modelPbB.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelPbKb.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelPbMb.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelPbGb.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelPbTb.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "\n",
        "  #Time\n",
        "  ####Seconds\n",
        "  modelSecMin.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelSecHour.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelSecDay.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelSecWeek.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelSecMonth.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  ####Minutes\n",
        "  modelMinSec.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelMinHour.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelMinDay.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelMinWeek.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelMinMonth.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  ####Hours\n",
        "  modelHourSec.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelHourMin.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelHourDay.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelHourWeek.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelHourMonth.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  ####Days\n",
        "  modelDaySec.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelDayMin.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelDayHour.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelDayWeek.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelDayMonth.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  ####Weeks\n",
        "  modelWeekSec.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelWeekMin.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelWeekHour.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelWeekDay.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelWeekMonth.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  ####Years\n",
        "  modelMonthSec.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelMonthMin.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelMonthHour.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelMonthDay.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelMonthWeek.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "\n",
        "  #Area\n",
        "  ####Square Meters\n",
        "  modelSqrmSqrk.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelSqrmSqrml.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelSqrmSqrh.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelSqrmSqry.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelSqrmSqrf.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  ####Square Kilometers\n",
        "  modelSqrkSqrm.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelSqrkSqrh.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelSqrkSqrml.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelSqrkSqry.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelSqrkSqrf.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  ####Hectare\n",
        "  modelSqrhSqrm.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelSqrhSqrkm.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelSqrhSqrml.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelSqrhSqry.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelSqrhSqrf.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  ####Square Miles\n",
        "  modelSqrmlSqrm.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelSqrmlSqrkm.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelSqrmlSqrh.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelSqrmlSqry.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelSqrmlSqrf.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  ####Square Yard\n",
        "  modelSqrySqrm.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelSqrySqrkm.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelSqrySqrml.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelSqrySqrh.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelSqrySqrf.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  ####Square Foot\n",
        "  modelSqrfSqrm.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelSqrfSqrkm.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelSqrfSqrml.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelSqrfSqrh.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "  modelSqrfSqry.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "\n",
        "  #Gap Months\n",
        "  modelAccMonth.compile(optimizer=tf.keras.optimizers.Adam(2), loss='mean_squared_error')\n",
        "\n",
        "  print('\\033[1;42m\\033[37m ¡Modelos compilados correctamente! \\033[0m')\n",
        "except:\n",
        "  print('\\033[1;41m\\033[1;37m ¡Ha ocurrido un error o se interrumpio la acción! \\033[0m')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;42m\u001b[37m ¡Modelos compilados correctamente! \u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRp5REiUX_Sz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54392400-24b3-4ba1-bd34-92773ece1390"
      },
      "source": [
        "#Go to gym\n",
        "try:\n",
        "  #Equal\n",
        "  modelEqual.fit(centi, centi, epochs=50, verbose=0)\n",
        "\n",
        "  #Length\n",
        "  ####Centimeters\n",
        "  modelCentMet.fit(centi, meter, epochs=500, verbose=0)\n",
        "  modelCentKilomet.fit(centi, kilometer, epochs=500, verbose=0)\n",
        "  modelCentMile.fit(centi, mile, epochs=500, verbose=0)\n",
        "  modelCentYard.fit(centi, yard, epochs=500, verbose=0)\n",
        "  modelCentFoot.fit(centi, foot, epochs=500, verbose=0)\n",
        "  ####Meters\n",
        "  modelMetCent.fit(meter, centi, epochs=500, verbose=0)\n",
        "  modelMetKilomet.fit(meter, kilometer, epochs=500, verbose=0)\n",
        "  modelMetMile.fit(meter, mile, epochs=500, verbose=0)\n",
        "  modelMetYard.fit(meter, yard, epochs=500, verbose=0)\n",
        "  modelMetFoot.fit(meter, foot, epochs=500, verbose=0)\n",
        "  ####Kilometers\n",
        "  modelKilometCent.fit(kilometer, centi, epochs=500, verbose=0)\n",
        "  modelKilometMet.fit(kilometer, meter, epochs=500, verbose=0)\n",
        "  modelKilometMile.fit(kilometer, mile, epochs=500, verbose=0)\n",
        "  modelKilometYard.fit(kilometer, yard, epochs=500, verbose=0)\n",
        "  modelKilometFoot.fit(kilometer, foot, epochs=500, verbose=0)\n",
        "  ####Miles\n",
        "  modelMileCent.fit(mile, centi, epochs=500, verbose=0)\n",
        "  modelMileMet.fit(mile, meter, epochs=500, verbose=0)\n",
        "  modelMileKilomet.fit(mile, kilometer, epochs=500, verbose=0)\n",
        "  modelMileYard.fit(mile, yard, epochs=500, verbose=0)\n",
        "  modelMileFoot.fit(mile, foot, epochs=500, verbose=0)\n",
        "  ####Yards\n",
        "  modelYardCent.fit(yard, centi, epochs=500, verbose=0)\n",
        "  modelYardMet.fit(yard, meter, epochs=500, verbose=0)\n",
        "  modelYardKilomet.fit(yard, kilometer, epochs=500, verbose=0)\n",
        "  modelYardMile.fit(yard, mile, epochs=500, verbose=0)\n",
        "  modelYardFoot.fit(yard, foot, epochs=500, verbose=0)\n",
        "  ####Foot\n",
        "  modelFootCent.fit(foot, centi, epochs=500, verbose=0)\n",
        "  modelFootMet.fit(foot, meter, epochs=500, verbose=0)\n",
        "  modelFootKilomet.fit(foot, kilometer, epochs=500, verbose=0)\n",
        "  modelFootMile.fit(foot, mile, epochs=500, verbose=0)\n",
        "  modelFootYard.fit(foot, yard, epochs=500, verbose=0)\n",
        "\n",
        "  #Mass\n",
        "  ####Grams\n",
        "  modelGramKilog.fit(gram, kilogram, epochs=500, verbose=0)\n",
        "  modelGramTon.fit(gram, ton, epochs=500, verbose=0)\n",
        "  modelGramSton.fit(gram, stone, epochs=500, verbose=0)\n",
        "  modelGramLibra.fit(gram, libra, epochs=500, verbose=0)\n",
        "  modelGramOce.fit(gram, once, epochs=500, verbose=0)\n",
        "  ####Kilograms\n",
        "  modelKilogGram.fit(kilogram, gram, epochs=500, verbose=0)\n",
        "  modelKilogTon.fit(kilogram, kilogram, epochs=500, verbose=0)\n",
        "  modelKilogSton.fit(kilogram, stone, epochs=500, verbose=0)\n",
        "  modelKilogLibra.fit(kilogram, libra, epochs=500, verbose=0)\n",
        "  modelKilogOnce.fit(kilogram, once, epochs=500, verbose=0)\n",
        "  ####Tons\n",
        "  modelTonGram.fit(ton, gram, epochs=500, verbose=0)\n",
        "  modelTonKilog.fit(ton, kilogram, epochs=500, verbose=0)\n",
        "  modelTonSton.fit(ton, stone, epochs=500, verbose=0)\n",
        "  modelTonLibra.fit(ton, libra, epochs=500, verbose=0)\n",
        "  modelTonOnce.fit(ton, once, epochs=500, verbose=0)\n",
        "  ####Stones\n",
        "  modelStoneGram.fit(stone, gram, epochs=500, verbose=0)\n",
        "  modelStoneKilog.fit(stone, kilogram, epochs=500, verbose=0)\n",
        "  modelStoneTon.fit(stone, ton, epochs=500, verbose=0)\n",
        "  modelStoneLibra.fit(stone, libra, epochs=500, verbose=0)\n",
        "  modelStoneOnce.fit(stone, once, epochs=500, verbose=0)\n",
        "  ####Libras\n",
        "  modelLibraGram.fit(libra, gram, epochs=500, verbose=0)\n",
        "  modelLibraKilog.fit(libra, kilogram, epochs=500, verbose=0)\n",
        "  modelLibraTon.fit(libra, ton, epochs=500, verbose=0)\n",
        "  modelLibraSton.fit(libra, stone, epochs=500, verbose=0)\n",
        "  modelLibraOnce.fit(libra, once, epochs=500, verbose=0)\n",
        "  ####Onces\n",
        "  modelOnceGram.fit(once, gram, epochs=500, verbose=0)\n",
        "  modelOnceKilog.fit(once, kilogram, epochs=500, verbose=0)\n",
        "  modelOnceTon.fit(once, ton, epochs=500, verbose=0)\n",
        "  modelOnceSton.fit(once, stone, epochs=500, verbose=0)\n",
        "  modelOnceLibra.fit(once, libra, epochs=500, verbose=0)\n",
        "\n",
        "  #Storage\n",
        "  ####Bytes\n",
        "  modelBKb.fit(byte, kilobyte, epochs=500, verbose=0)\n",
        "  modelBMb.fit(byte, megabyte, epochs=500, verbose=0)\n",
        "  modelBGb.fit(byte, gigabyte, epochs=500, verbose=0)\n",
        "  modelBTb.fit(byte, terabyte, epochs=500, verbose=0)\n",
        "  modelBPb.fit(byte, petabyte, epochs=500, verbose=0)\n",
        "  ####Kylobytes\n",
        "  modelKbB.fit(kilobyte, byte, epochs=500, verbose=0)\n",
        "  modelKbMb.fit(kilobyte, megabyte, epochs=500, verbose=0)\n",
        "  modelKbGb.fit(kilobyte, gigabyte, epochs=500, verbose=0)\n",
        "  modelKbTb.fit(kilobyte, terabyte, epochs=500, verbose=0)\n",
        "  modelKbPb.fit(kilobyte, petabyte, epochs=500, verbose=0)\n",
        "  ####Megabytes\n",
        "  modelMbB.fit(megabyte, byte, epochs=500, verbose=0)\n",
        "  modelMbKb.fit(megabyte, kilobyte, epochs=500, verbose=0)\n",
        "  modelMbGb.fit(megabyte, gigabyte, epochs=500, verbose=0)\n",
        "  modelMbTb.fit(megabyte, terabyte, epochs=500, verbose=0)\n",
        "  modelMbPb.fit(megabyte, petabyte, epochs=500, verbose=0)\n",
        "  ####Gigabytes\n",
        "  modelGbB.fit(gigabyte, byte, epochs=500, verbose=0)\n",
        "  modelGbKb.fit(gigabyte, kilobyte, epochs=500, verbose=0)\n",
        "  modelGbMb.fit(gigabyte, megabyte, epochs=500, verbose=0)\n",
        "  modelGbTb.fit(gigabyte, terabyte, epochs=500, verbose=0)\n",
        "  modelGbPb.fit(gigabyte, petabyte, epochs=500, verbose=0)\n",
        "  ####Terabytes\n",
        "  modelTbB.fit(terabyte, byte, epochs=500, verbose=0)\n",
        "  modelTbKb.fit(terabyte, kilobyte, epochs=500, verbose=0)\n",
        "  modelTbMb.fit(terabyte, megabyte, epochs=500, verbose=0)\n",
        "  modelTbGb.fit(terabyte, gigabyte, epochs=500, verbose=0)\n",
        "  modelTbPb.fit(terabyte, petabyte, epochs=500, verbose=0)\n",
        "  ####Petabytes\n",
        "  modelPbB.fit(petabyte, byte, epochs=500, verbose=0)\n",
        "  modelPbKb.fit(petabyte, kilobyte, epochs=500, verbose=0)\n",
        "  modelPbMb.fit(petabyte, megabyte, epochs=500, verbose=0)\n",
        "  modelPbGb.fit(petabyte, gigabyte, epochs=500, verbose=0)\n",
        "  modelPbTb.fit(petabyte, terabyte, epochs=500, verbose=0)\n",
        "\n",
        "  #Time\n",
        "  ####Seconds\n",
        "  modelSecMin.fit(second, minute, epochs=500, verbose=0)\n",
        "  modelSecHour.fit(second, hour, epochs=500, verbose=0)\n",
        "  modelSecDay.fit(second, day, epochs=500, verbose=0)\n",
        "  modelSecWeek.fit(second, week, epochs=500, verbose=0)\n",
        "  modelSecMonth.fit(second, month, epochs=500, verbose=0)\n",
        "  ####Minutes\n",
        "  modelMinSec.fit(minute, second, epochs=500, verbose=0)\n",
        "  modelMinHour.fit(minute, hour, epochs=500, verbose=0)\n",
        "  modelMinDay.fit(minute, day, epochs=500, verbose=0)\n",
        "  modelMinWeek.fit(minute, week, epochs=500, verbose=0)\n",
        "  modelMinMonth.fit(minute, month, epochs=500, verbose=0)\n",
        "  ####Hours\n",
        "  modelHourSec.fit(hour, second, epochs=500, verbose=0)\n",
        "  modelHourMin.fit(hour, minute, epochs=500, verbose=0)\n",
        "  modelHourDay.fit(hour, day, epochs=500, verbose=0)\n",
        "  modelHourWeek.fit(hour, week, epochs=500, verbose=0)\n",
        "  modelHourMonth.fit(hour, month, epochs=500, verbose=0)\n",
        "  ####Days\n",
        "  modelDaySec.fit(day, second, epochs=500, verbose=0)\n",
        "  modelDayMin.fit(day, minute, epochs=500, verbose=0)\n",
        "  modelDayHour.fit(day, hour, epochs=500, verbose=0)\n",
        "  modelDayWeek.fit(day, week, epochs=500, verbose=0)\n",
        "  modelDayMonth.fit(day, month, epochs=500, verbose=0)\n",
        "  ####Weeks\n",
        "  modelWeekSec.fit(week, second, epochs=500, verbose=0)\n",
        "  modelWeekMin.fit(week, minute, epochs=500, verbose=0)\n",
        "  modelWeekHour.fit(week, hour, epochs=500, verbose=0)\n",
        "  modelWeekDay.fit(week, day, epochs=500, verbose=0)\n",
        "  modelWeekMonth.fit(week, month, epochs=500, verbose=0)\n",
        "  ####Months\n",
        "  modelMonthSec.fit(month, second, epochs=500, verbose=0)\n",
        "  modelMonthMin.fit(month, minute, epochs=500, verbose=0)\n",
        "  modelMonthHour.fit(month, hour, epochs=500, verbose=0)\n",
        "  modelMonthDay.fit(month, day, epochs=500, verbose=0)\n",
        "  modelMonthWeek.fit(month, week, epochs=500, verbose=0)\n",
        "\n",
        "  #Area\n",
        "  ####Square Meters\n",
        "  modelSqrmSqrk.fit(sqrMeter, sqrKilometer, epochs=500, verbose=0)\n",
        "  modelSqrmSqrh.fit(sqrMeter, sqrHectare, epochs=500, verbose=0)\n",
        "  modelSqrmSqrml.fit(sqrMeter, sqrMile, epochs=500, verbose=0)\n",
        "  modelSqrmSqry.fit(sqrMeter, sqrYard, epochs=500, verbose=0)\n",
        "  modelSqrmSqrf.fit(sqrMeter, sqrFoot, epochs=500, verbose=0)\n",
        "  ####Square Kilometers\n",
        "  modelSqrkSqrm.fit(sqrKilometer, sqrMeter, epochs=500, verbose=0)\n",
        "  modelSqrkSqrh.fit(sqrKilometer, sqrHectare, epochs=500, verbose=0)\n",
        "  modelSqrkSqrml.fit(sqrKilometer, sqrMile, epochs=500, verbose=0)\n",
        "  modelSqrkSqry.fit(sqrKilometer, sqrYard, epochs=500, verbose=0)\n",
        "  modelSqrkSqrf.fit(sqrKilometer, sqrFoot, epochs=500, verbose=0)\n",
        "  ####Hectares\n",
        "  modelSqrhSqrm.fit(sqrHectare, sqrMeter, epochs=500, verbose=0)\n",
        "  modelSqrhSqrkm.fit(sqrHectare, sqrKilometer, epochs=500, verbose=0)\n",
        "  modelSqrhSqrml.fit(sqrHectare, sqrMile, epochs=500, verbose=0)\n",
        "  modelSqrhSqry.fit(sqrHectare, sqrYard, epochs=500, verbose=0)\n",
        "  modelSqrhSqrf.fit(sqrHectare, sqrFoot, epochs=500, verbose=0)\n",
        "  ####Square Miles\n",
        "  modelSqrmlSqrm.fit(sqrMile, sqrMeter, epochs=500, verbose=0)\n",
        "  modelSqrmlSqrkm.fit(sqrMile, sqrKilometer, epochs=500, verbose=0)\n",
        "  modelSqrmlSqrh.fit(sqrMile, sqrHectare, epochs=500, verbose=0)\n",
        "  modelSqrmlSqry.fit(sqrMile, sqrYard, epochs=500, verbose=0)\n",
        "  modelSqrmlSqrf.fit(sqrMile, sqrFoot, epochs=500, verbose=0)\n",
        "  ####Square Yards\n",
        "  modelSqrySqrm.fit(sqrYard, sqrMeter, epochs=500, verbose=0)\n",
        "  modelSqrySqrkm.fit(sqrYard, sqrKilometer, epochs=500, verbose=0)\n",
        "  modelSqrySqrh.fit(sqrYard, sqrHectare, epochs=500, verbose=0)\n",
        "  modelSqrySqrml.fit(sqrYard, sqrMile, epochs=500, verbose=0)\n",
        "  modelSqrySqrf.fit(sqrYard, sqrFoot, epochs=500, verbose=0)\n",
        "  ####Square Foots\n",
        "  modelSqrfSqrm.fit(sqrFoot, sqrMeter, epochs=500, verbose=0)\n",
        "  modelSqrfSqrkm.fit(sqrFoot, sqrKilometer, epochs=500, verbose=0)\n",
        "  modelSqrfSqrh.fit(sqrFoot, sqrHectare, epochs=500, verbose=0)\n",
        "  modelSqrfSqrml.fit(sqrFoot, sqrMile, epochs=500, verbose=0)\n",
        "  modelSqrfSqry.fit(sqrFoot, sqrYard, epochs=500, verbose=0)\n",
        "\n",
        "  #Gap months\n",
        "  modelAccMonth.fit(inputMonth, accumMonth, epochs=500, verbose=0)\n",
        "\n",
        "  print('\\033[1;42m\\033[37m ¡Modelos entrenados correctamente! \\033[0m')\n",
        "except:\n",
        "  print('\\033[1;41m\\033[1;37m ¡Ha ocurrido un error o se interrumpio la acción! \\033[0m')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;42m\u001b[37m ¡Modelos entrenados correctamente! \u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8BF0GeVA1RX",
        "outputId": "65c71b5a-d8fc-45ff-d0ad-775b7f39ec96"
      },
      "source": [
        "#Converter\n",
        "menu=[\n",
        "     ['Meses Acumulados', modelAccMonth], #Accumulated Months\n",
        "     ['Longitud', #Length\n",
        "      ['Centímetros', modelEqual, modelCentMet, modelCentKilomet, modelCentMile, modelCentYard, modelCentFoot], #Centimeters\n",
        "      ['Metros', modelMetCent, modelEqual, modelMetKilomet, modelMetMile, modelMetYard, modelMetFoot], #Meters\n",
        "      ['Kilómetros', modelKilometCent, modelKilometMet, modelEqual, modelKilometMile, modelKilometYard, modelKilometFoot], #Kilometers\n",
        "      ['Millas', modelMileCent, modelMileMet, modelMileKilomet, modelEqual, modelMileYard, modelMileFoot], #Miles\n",
        "      ['Yardas', modelYardCent, modelYardMet, modelYardKilomet, modelYardMile, modelEqual, modelYardCent], #Yards\n",
        "      ['Pies', modelFootCent, modelFootMet, modelFootKilomet, modelFootMile, modelFootYard, modelEqual]], #Foots\n",
        "     ['Masa', #Mass\n",
        "      ['Gramos', modelEqual, modelGramKilog, modelGramTon, modelGramSton, modelGramLibra, modelGramOce], #Grams\n",
        "      ['Kilogramos', modelKilogGram, modelEqual, modelKilogTon, modelKilogSton, modelKilogLibra, modelKilogOnce], #Kilograms\n",
        "      ['Toneladas', modelTonGram, modelTonKilog, modelEqual, modelTonSton, modelTonLibra, modelTonOnce], #Tons\n",
        "      ['\"Stones\"', modelStoneGram, modelStoneKilog, modelStoneTon, modelEqual, modelStoneLibra, modelStoneOnce], #Stones\n",
        "      ['Libras', modelLibraGram, modelLibraKilog, modelLibraTon, modelLibraSton, modelEqual, modelLibraOnce], #Libras\n",
        "      ['Onzas', modelOnceGram, modelOnceKilog, modelOnceTon, modelOnceSton, modelOnceLibra, modelEqual]], #Onces\n",
        "     ['Almacenamiento', #Storage\n",
        "      ['Bytes', modelEqual, modelBKb, modelBMb, modelBGb, modelBTb, modelBPb], #Bytes\n",
        "      ['Kilobytes', modelKbB, modelEqual, modelKbMb, modelKbGb, modelKbTb, modelKbPb], #Kilobytes\n",
        "      ['Megabytes', modelMbB, modelMbKb, modelEqual, modelMbGb, modelMbTb, modelMbPb], #Megabytes\n",
        "      ['Gigabytes', modelGbB, modelGbKb, modelGbMb, modelEqual, modelGbTb, modelGbPb], #Gigabytes\n",
        "      ['Terabytes', modelTbB, modelTbKb, modelTbMb, modelTbGb, modelEqual, modelTbPb], #Terabytes\n",
        "      ['Petabytes', modelPbB, modelPbKb, modelPbMb, modelPbGb, modelPbTb, modelEqual]], #Petabytes\n",
        "     ['Tiempo', #Times\n",
        "      ['Segundos', modelEqual, modelSecMin, modelSecHour, modelSecDay, modelSecWeek, modelSecMonth], #Seconds\n",
        "      ['Minutos', modelMinSec, modelEqual, modelMinHour, modelMinDay, modelMinWeek, modelMinMonth], #Minutes\n",
        "      ['Horas', modelHourSec, modelHourMin, modelEqual, modelHourDay, modelHourWeek, modelHourMonth], #Hours\n",
        "      ['Días', modelDaySec, modelDayMin, modelDayHour, modelEqual, modelDayWeek, modelDayMonth], #Days\n",
        "      ['Semanas', modelWeekSec, modelWeekMin, modelWeekHour, modelWeekDay, modelEqual, modelWeekMonth], #Weeks\n",
        "      ['Meses', modelMonthSec, modelMonthMin, modelMonthHour, modelMonthDay, modelMonthWeek, modelEqual]], #Months\n",
        "     ['Area', #Area\n",
        "      ['Metros Cuadrados', modelEqual, modelSqrmSqrk, modelSqrmSqrh, modelSqrmSqrml, modelSqrmSqry, modelSqrmSqrf], #Squre Meters\n",
        "      ['Kilómetros Cuadrados', modelSqrkSqrm, modelEqual, modelSqrkSqrh, modelSqrkSqrml, modelSqrkSqry, modelSqrkSqrf], #Square Kilometers\n",
        "      ['Hectareas', modelSqrhSqrm, modelSqrhSqrkm, modelEqual, modelSqrhSqrml, modelSqrhSqry, modelSqrhSqrf], #Hectare\n",
        "      ['Millas Cuadradas', modelSqrmlSqrm, modelSqrmlSqrkm, modelSqrmlSqrh, modelEqual, modelSqrmlSqry, modelSqrmlSqrf], #Square Miles\n",
        "      ['Yardas Cuadradas', modelSqrySqrm, modelSqrySqrkm, modelSqrySqrh, modelSqrySqrml, modelEqual, modelSqrySqrf], #Square Yards\n",
        "      ['Pies Cuadrados', modelSqrfSqrm, modelSqrfSqrkm, modelSqrfSqrh, modelSqrfSqrml, modelSqrfSqry, modelEqual]] #Square Foots\n",
        "]\n",
        "stop = False\n",
        "\n",
        "print('¿Qué desea convertir?')\n",
        "print(f'1) {menu[1][0]}\\n2) {menu[2][0]}\\n3) {menu[3][0]}\\n4) {menu[4][0]}\\n5) {menu[5][0]}')\n",
        "while stop == False:\n",
        "  opt = int(input('Conversor: '))\n",
        "\n",
        "  if opt >= 1 and opt <= 5:\n",
        "    stop = True\n",
        "    print(f'\\n\\033[0;37;42m {menu[opt][0]} \\033[0m')\n",
        "  else:\n",
        "    print('\\033[1;41m\\033[1;37m Ingresar un valor entre 1 y 5 \\033[0m')\n",
        "print('')\n",
        "stop = False\n",
        "\n",
        "print(f'1) {menu[opt][1][0]}\\n2) {menu[opt][2][0]}\\n3) {menu[opt][3][0]}\\n4) {menu[opt][4][0]}\\n5) {menu[opt][5][0]}\\n6) {menu[opt][6][0]}\\n')\n",
        "uFrom = int(input('De: '))\n",
        "uTo = int(input('A: '))\n",
        "val = float(input('Cantidad: '))\n",
        "print('')\n",
        "\n",
        "result = menu[opt][uFrom][uTo].predict([val])\n",
        "\n",
        "print(f'\\033[0;37;42m {val} {menu[opt][uFrom][0]} a {menu[opt][uTo][0]} es: \\033[1;4m{result[0][0]:.4f} \\033[0m')"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¿Qué desea convertir?\n",
            "1) Longitud\n",
            "2) Masa\n",
            "3) Almacenamiento\n",
            "4) Tiempo\n",
            "5) Area\n",
            "Conversor: 4\n",
            "\n",
            "\u001b[0;37;42m Tiempo \u001b[0m\n",
            "\n",
            "1) Segundos\n",
            "2) Minutos\n",
            "3) Horas\n",
            "4) Días\n",
            "5) Semanas\n",
            "6) Meses\n",
            "\n",
            "De: 6\n",
            "A: 5\n",
            "Cantidad: 3\n",
            "\n",
            "\u001b[0;37;42m 3.0 Meses a Semanas es: \u001b[1;4m13.4155 \u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xQiEg9M-qzv",
        "outputId": "89202525-a242-4a20-d4fc-bb11f11e61ec"
      },
      "source": [
        "print(menu[0][0])\n",
        "val = int(input('Cantidad de meses acumulados: '))\n",
        "result = menu[0][1].predict([val])\n",
        "print(f'\\n\\033[0;37;42m {val} meses acumulados son: \\033[1;4m{result[0][0]:.4f} \\033[0m')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meses Acumulados\n",
            "Cantidad de meses acumulados: 8\n",
            "\n",
            "\u001b[0;37;42m 8 meses acumulados son: \u001b[1;4m39.9839 \u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hz3ld8kUTHXd"
      },
      "source": [
        "#En caso de ser necesario reentrenar una AI, reemplazar el nombre del modelo y datosa de entrada y salida\n",
        "modelSqrmSqrf.fit(sqrMeter, sqrFoot, epochs=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCFypdUUeHm0",
        "outputId": "345eed10-b868-4afe-b822-1b41eb433c3d"
      },
      "source": [
        "x = modelSqrmSqrf.predict([400])\n",
        "print([x])"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[4305.5596]], dtype=float32)]\n"
          ]
        }
      ]
    }
  ]
}